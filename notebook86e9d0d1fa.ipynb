{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n\ncsv_file_path = '/kaggle/input/sms-spam-collection-dataset/spam.csv'  # Adjust the file path if necessary\ndf = pd.read_csv(csv_file_path, encoding='ISO-8859-1')\n\n\nprint(\"First few rows of the dataset:\")\nprint(df.head())\n\n\nprint(\"\\nColumn names in the dataset:\")\nprint(df.columns)\n\n\ndf = df[['v1', 'v2']]\ndf.columns = ['label', 'message']\n\n\ndf['label'] = df['label'].map({'ham': 0, 'spam': 1})\n\n\nprint(\"\\nMissing values in the dataset:\")\nprint(df.isnull().sum())\n\n\nX_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n\n\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n\ndef train_evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    print(f\"Evaluating {model.__class__.__name__}:\")\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n\nnb_classifier = MultinomialNB()\ntrain_evaluate_model(nb_classifier, X_train_tfidf, y_train, X_test_tfidf, y_test)\n\n\nlog_reg_classifier = LogisticRegression(random_state=42)\ntrain_evaluate_model(log_reg_classifier, X_train_tfidf, y_train, X_test_tfidf, y_test)\n\n\nsvm_classifier = SVC(kernel='linear', random_state=42)\ntrain_evaluate_model(svm_classifier, X_train_tfidf, y_train, X_test_tfidf, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T10:11:00.717535Z","iopub.execute_input":"2024-06-19T10:11:00.718604Z","iopub.status.idle":"2024-06-19T10:11:01.840781Z","shell.execute_reply.started":"2024-06-19T10:11:00.718530Z","shell.execute_reply":"2024-06-19T10:11:01.839391Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"First few rows of the dataset:\n     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  \n\nColumn names in the dataset:\nIndex(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n\nMissing values in the dataset:\nlabel      0\nmessage    0\ndtype: int64\nEvaluating MultinomialNB:\nConfusion Matrix:\n [[965   0]\n [ 37 113]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98       965\n           1       1.00      0.75      0.86       150\n\n    accuracy                           0.97      1115\n   macro avg       0.98      0.88      0.92      1115\nweighted avg       0.97      0.97      0.96      1115\n\nAccuracy Score: 0.9668161434977578\n\n============================================================\n\nEvaluating LogisticRegression:\nConfusion Matrix:\n [[964   1]\n [ 35 115]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98       965\n           1       0.99      0.77      0.86       150\n\n    accuracy                           0.97      1115\n   macro avg       0.98      0.88      0.92      1115\nweighted avg       0.97      0.97      0.97      1115\n\nAccuracy Score: 0.967713004484305\n\n============================================================\n\nEvaluating SVC:\nConfusion Matrix:\n [[963   2]\n [ 17 133]]\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99       965\n           1       0.99      0.89      0.93       150\n\n    accuracy                           0.98      1115\n   macro avg       0.98      0.94      0.96      1115\nweighted avg       0.98      0.98      0.98      1115\n\nAccuracy Score: 0.9829596412556054\n\n============================================================\n\n","output_type":"stream"}]}]}